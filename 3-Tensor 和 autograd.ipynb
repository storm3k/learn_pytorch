{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 基础操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从接口角度讲，对 tensor 的操作可以分为两类\n",
    "- `torch.function`, 如 `torch.save`\n",
    "- `tensor.function`, 如 `tensor.view`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从存储的角度讲，对 tensor 的操作可以分为两类\n",
    "- 不会修改自身的数据 `a.add(b)`\n",
    "- 会修改自身的数据 `a.add_(b)` inplace 方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**创建 Tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的新建 tensor 方法\n",
    "\n",
    "| 函数 | 功能 |\n",
    "| :------| ------: |\n",
    "| Tesor(*sizes)  | 基础构造函数 |\n",
    "| ones(*sizes) | 全 1 Tensor |\n",
    "| zeros(*sizes) | 全 0 Tensor |\n",
    "| eye(*sizes) | 对角线为 1, 其他为 0 |\n",
    "| arange(s, e, steps) | 从 s 到 e, 步长为 step |\n",
    "| linspace(s, e, steps) | 从 s 到 e, 均匀切分成 steps 份 |\n",
    "| rand/randn(*sizes) | 均匀 / 标准正态分布 |\n",
    "| normal(mean, std) / uniform(form, to) | 正态分布 / 均匀分布 |\n",
    "| randperm(m) | 随机排列 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1866e+13,  3.0735e-41,  4.9656e+28],\n",
       "        [ 4.5439e+30,  5.7453e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定 tensor 的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a  # 数值取决于内存空间的状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 list 的数据创建 tensor\n",
    "b = t.Tensor([[1,2,3], [4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist()  # 把 tensor 转为 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.size()` 返回 `torch.Size` 对象，它是 tuple 的子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel()  # b 中元素总个数， 2*3 等价于 b.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5843e+09,  4.5566e-41, -1.8455e+13],\n",
      "        [ 3.0735e-41,  4.4842e-44,  0.0000e+00]])\n",
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个和 b 形状一样的 tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为 2 和 3 的 tensor\n",
    "d = t.Tensor((2, 3))\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以用 `tensor.shape` 直接查看 tensor 的形状\n",
    "\n",
    "`tensor.shape` 等价于 `tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "\n",
    "`t.Tensor(*sizes)`创建 tensor 时，系统不会马上分配空间，\n",
    "\n",
    "只会计算剩余的内存是否够用，使用到 tensor 时才会分配，\n",
    "\n",
    "而其他操作都是在创建完成 tensor 后马上进行空间分配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1608,  1.6276, -0.6581],\n",
       "        [-0.5305,  0.2952,  0.0953]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 0, 3, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5)  # 长度为 5 的随机排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3)  # 对角线为 1, 不要求行列数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常用 Tensor 操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tensor.view`方法可以调整 tensor 的形状，\n",
    "\n",
    "但必须保证调整前后元素个数一致。\n",
    "\n",
    "view 不会修改自身的数据，返回的新的 tensor 与源 tensor 共享内存\n",
    "\n",
    "实际应用中经常需要添加或减少某一维度，用`squeeze`和`unsqueeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "print(a)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 3)  # 当某一维为 -1 的时候，会自动计算它的大小\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1)  # 在第 1 维(下标从0开始)上增加 “1”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c.squeeze(0)  # 压缩第 0 维的 “1”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze()  # 把所有维度为 “1” 的压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b  # a 和 b 共享内存，修改了a, b也变了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resize`可以用来调整 size，与 view 不同，它可以修改 tensor 尺寸\n",
    "\n",
    "如果新尺寸超过了原尺寸，则会分配新的内存空间，如果小于，则之前的数据会保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[              0,             100,               2],\n",
       "        [              3,               4,               5],\n",
       "        [            143, 139662924089920,              80]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3)  # 级的数据依旧保存着，多出的数据会分配新的空间\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如无特殊说明，索引出来的结果与原 tensor 共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.6869e-02, -2.0206e-01, -9.6794e-04, -1.1772e+00],\n",
       "        [ 1.1646e+00, -1.3940e-01, -4.5974e-01,  3.1210e-01],\n",
       "        [-3.8002e-01,  1.4402e+00, -1.2241e+00,  1.4215e-01]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.6869e-02, -2.0206e-01, -9.6794e-04, -1.1772e+00])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]  # 第 0 行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0669,  1.1646, -0.3800])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0]  # 第 0 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0010)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]  # 第 0 行  第 2 个元素 等价于 a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1772)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1]  # 第 0 行最后一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.6869e-02, -2.0206e-01, -9.6794e-04, -1.1772e+00],\n",
       "        [ 1.1646e+00, -1.3940e-01, -4.5974e-01,  3.1210e-01]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2]  # 前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0669, -0.2021],\n",
       "        [ 1.1646, -0.1394]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2]  # 前两行，第 0,1 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0669, -0.2021]])\n",
      "tensor([ 0.0669, -0.2021])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2])  # 第 0 行，前两列\n",
    "print(a[0, :2])  # 注意两者的区别： 形状不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 1, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1  # 返回一个 ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1646, 1.4402])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>1]  # 等价于 a.masked_select(a>1)\n",
    "# 选择结果与原 tensor 不共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.6869e-02, -2.0206e-01, -9.6794e-04, -1.1772e+00],\n",
       "        [ 1.1646e+00, -1.3940e-01, -4.5974e-01,  3.1210e-01]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0, 1])]  # 第 0 行和第 1 列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常用的选择函数\n",
    "\n",
    "| 函数 | 功能 |\n",
    "| :------| ------: |\n",
    "| index_select(input, dim, index)  | 在指定维度 dim 上选取，例如选取某些行、某些列 |\n",
    "| masked_select(input, mask) | 使用 ByteTensor 进行选取 |\n",
    "| non_zero(input) | 非 0 远古三的下标 |\n",
    "| gather(input, dim, index) | 根据 index, 在 dim 维度上选取数据，输出的 size 与 index 一样 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gather 是一个比较复杂的操作，对一个二维 tensor，输出的每个元素如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[i][j] = input_[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input_[i][index[i][j]]  # dim=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a  #　注意此时　a 为 long, 要转化 a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3, 2, 1, 0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素, 注意与上边的不同\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  3.],\n",
       "        [ 0.,  5.,  6.,  0.],\n",
       "        [ 0.,  9., 10.,  0.],\n",
       "        [12.,  0.,  0., 15.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4, 4)\n",
    "c.scatter_(1, index, b.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0]  #依旧是tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0].item()  # python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**高级索引**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级索引可以看成是**普通索引操作的扩展**，但是高级索引操作的结果**一般不和原始的Tensor共享内存**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0, 27).view(3, 3, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,2], [1,2], [2, 0]]  # x[1,1,2]和 x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 类型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。**默认的tensor**是`FloatTensor`，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对**分析内存占用**很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "| Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n",
    "| ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n",
    "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n",
    "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n",
    "| 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n",
    "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n",
    "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n",
    "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n",
    "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现，此外还可以使用`tensor.to(device)`。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。`torch.*_like(tensora)` 可以生成和`tensora`拥有同样属性(类型，形状，cpu/gpu)的新tensor。 `tensor.new_*(new_shape)` 新建一个不同形状的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2, 3)\n",
    "a.dtype  # 现在a是DoubleTensor,dtype是float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把 a 转成 FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float()\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.6717e-310, 6.8498e+180, 2.0926e+262],\n",
       "        [2.1049e+262, 3.9466e+180, 2.0257e-322]], dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new(2, 3)  # 等价于torch.DoubleTensor(2,3)，建议使用a.new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a)  #等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a, dtype=t.int16)  # 可以修改某些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7092, 0.4930, 0.6943],\n",
       "        [0.2902, 0.6740, 0.1555]], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(4, 5, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_tensor([3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**逐元素操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "​\n",
    "表3-4: 常见的逐元素操作\n",
    "​\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "​\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "​\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3  # 等价于t.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2  # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.8415,  0.9093],\n",
       "        [ 0.1411, -0.7568, -0.9589]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.sin_()  # 效果同 a = a.sin();b=a ,但是更高效节省显存\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**归并操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此类操作会使**输出形状**小于**输入形状**，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  3],\n",
       "        [ 3,  7, 12]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1)  # 沿着行累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**比较**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表3-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  6.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 6.,  3.,  0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 12., 15.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a > b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([15.,  6.]), tensor([0, 0]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(b, dim=1)\n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 12., 15.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**线性代数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表3-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  9.],\n",
       "        [ 3., 12.],\n",
       "        [ 6., 15.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2, 3], dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a)  # 也可以直接将numpy对象传入Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]=100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy()  # a, b, c三个对象共享内存\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "a.dtype  # 注意和上面的a的区别（dtype不是float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a)  # 此处进行拷贝，不共享内存\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a)  # 注意c的类型（DoubleTensor）\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b  # b与a不共享内存，所以即使a改变了，b也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c  # c与a共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 不论输入的类型是什么，t.tensor都会进行数据拷贝，不会共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = t.tensor(a)  # tensor 首字母小写！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0, 0] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**广播法则**(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时**不会占用额外的内存/显存**。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的**长度一致**，**要么其中一个为1**，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时**沿此维度复制扩充成一样的形状**\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`，或者tensor[None],：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步： a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#        即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步: a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#        可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a[None].expand(2, 3, 2) + b.expand(2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(100000000, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 内部结构\n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为**头信息区(Tensor)**和**存储区(Storage)**，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而**真正的数据则保存成连续数组**。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 100\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:]\n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94555535863360, 94555535863344)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr()  # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)\n",
    "# 此处是 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  100, -100,    3,    4,    5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100  # c[0]的内存地址对应a[2]的内存地址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of data type 6 but got data type 4 for argument #2 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-37b04ae46c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6666\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of data type 6 but got data type 4 for argument #2 'source'"
     ]
    }
   ],
   "source": [
    "# RuntimeError: Expected object of data type 6 but got data type 4 for argument #2 'source'\n",
    "d = t.Tensor(c.storage())\n",
    "d[0] = 6666\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()\n",
    "# (0, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2] # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是**修改了tensor的头信息**。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们**变成连续的数据**，该方法会使数据复制一份，不再与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 其它有关Tensor的话题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU/CPU\n",
    "tensor可以很随意的在gpu/cpu上传输。使用`tensor.cuda(device_id)`或者`tensor.cpu()`。另外一个更通用的方法是`tensor.to(device)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = t.randn(3, 4, device=t.device('cuda:0'))\n",
    "    # 等价于 a.t.randn(3,4).cuda(1)\n",
    "    # 但是前者更快\n",
    "    a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0211, -0.7640, -1.1259, -1.6994],\n",
       "        [ 0.7734, -0.6615, -3.2119,  1.7461],\n",
       "        [-0.0663,  1.7544, -0.9747,  0.1721]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device('cpu')\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(a.device)\n",
    "print(a.to(device).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "- 尽量使用`tensor.to(device)`, 将`device`设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU\n",
    "- 数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   向量化\n",
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i, j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739 µs ± 31.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 12.88 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "8.14 µs ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见二者有超过几十倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯，千万避免对较大的tensor进行逐元素遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来**限制PyTorch所占用的CPU数目**。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19999999, dtype=torch.int32) tensor(19999998, dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(19999999), tensor(19999998))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000).int()\n",
    "print(a[-1], a[-2])\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000000, out=b)\n",
    "b[-1], b[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2384, -0.0341,  1.3250],\n",
       "        [ 0.7597,  0.3193, -0.4554]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2384417057, -0.0340893641,  1.3250156641],\n",
       "        [ 0.7597360015,  0.3193188012, -0.4553743899]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "device = t.device('cpu')  #如果你想用gpu，改成t.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000)\n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    \"\"\"产生随机数据：y=x*2+3，加上了一些噪声\"\"\"\n",
    "    x = t.rand(batch_size, 1, device=device) * 5\n",
    "    y = x * 2 + 3 + t.randn(batch_size, 1, device=device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcdaaddbe80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data(batch_size=16)\n",
    "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  2.0905983448028564 b:  3.170895576477051\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1).to(device)\n",
    "b = t.zeros(1, 1).to(device)\n",
    "\n",
    "lr = 0.02  # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=4)\n",
    "    \n",
    "    # forward: 计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y)  # x@W等价于x.mm(w);for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2  # 均方误差\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # backward: 手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii % 50 == 0:\n",
    "        \n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 6).float().view(-1, 1)\n",
    "        y = x.mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy())  # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=32)\n",
    "        plt.scatter(x2.numpy(), y2.numpy())  # true data\n",
    "        \n",
    "        plt.xlim(0, 5)\n",
    "        plt.ylim(0, 13)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print('w: ', w.item(), 'b: ', b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图(Computation Graph)是现代深度学习框架如PyTorch和TensorFlow等的核心，其为高效自动求导算法——反向传播(Back Propogation)提供了理论支持，了解计算图在实际写程序过程中会有极大的帮助。\n",
    "http://colah.github.io/posts/2015-08-Backprop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch在autograd模块中实现了计算图的相关功能，autograd中的核心数据结构是Variable。从`v0.4`版本起，Variable和Tensor**合并**。我们可以认为需要求导(requires_grad)的tensor即Variable. autograd记录对tensor的操作记录用来构建计算图。\n",
    "\n",
    "Variable提供了大部分tensor支持的函数，但其不支持部分`inplace`函数，因这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算反向传播梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的`backward`方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。\n",
    "\n",
    "`variable.backward(gradient=None, retain_graph=None, create_graph=None)`主要有如下参数：\n",
    "\n",
    "- `grad_variables`：形状与variable一致，对于`y.backward()`，grad_variables相当于链式法则${dz \\over dx}={dz \\over dy} \\times {dy \\over dx}$中的$\\textbf {dz} \\over \\textbf {dy}$。grad_variables也可以是tensor或序列。\n",
    "- `retain_graph`：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。\n",
    "- `create_graph`：对反向传播过程再次构建计算图，可通过`backward of backward`实现求高阶导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2357, -2.6218, -0.2059,  1.4089],\n",
       "        [ 1.5111,  1.6449,  1.2044, -0.4726],\n",
       "        [-1.2300, -2.5441, -0.8108,  0.3973]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在创建tensor的时候指定requires_grad\n",
    "a = t.randn(3, 4, requires_grad=True)\n",
    "# 或者\n",
    "a = t.randn(3, 4).requires_grad_()\n",
    "# 或者\n",
    "a = t.randn(3, 4)\n",
    "a.requires_grad=True\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.zeros(3, 4).requires_grad_()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2357, -2.6218, -0.2059,  1.4089],\n",
       "        [ 1.5111,  1.6449,  1.2044, -0.4726],\n",
       "        [-1.2300, -2.5441, -0.8108,  0.3973]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可写成c = a + b\n",
    "c = a.add(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.sum()\n",
    "d.backward()  # 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d  # d还是一个requires_grad=True的tensor,对它的操作需要慎重\n",
    "d.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导，\n",
    "# 因此c的requires_grad属性会自动设为True\n",
    "a.requires_grad, b.requires_grad, c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由用户创建的variable属于叶子节点，对应的grad_fn是None\n",
    "a.is_leaf, b.is_leaf, c.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c.grad是None, 因c不是叶子节点，它的梯度是用来计算a的梯度\n",
    "# 所以虽然c.requires_grad = True,但其梯度计算完之后即被释放\n",
    "c.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算下面这个函数的导函数：\n",
    "$$\n",
    "y = x^2\\bullet e^x\n",
    "$$\n",
    "它的导函数是：\n",
    "$$\n",
    "{dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n",
    "$$\n",
    "来看看autograd的计算结果与手动求导计算结果的误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"计算 y\"\"\"\n",
    "    y = x**2 * t.exp(x)\n",
    "    return y\n",
    "\n",
    "def gradf(x):\n",
    "    \"\"\"手动求导\"\"\"\n",
    "    dx = 2*x*t.exp(x) + x**2*t.exp(x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6529e-06, 5.3157e-01, 3.2374e-01, 6.6563e+00],\n",
       "        [7.5196e+00, 5.4100e-01, 5.2085e-01, 5.2966e-01],\n",
       "        [1.7214e-01, 2.1973e-01, 2.5474e+00, 1.1979e+00]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.randn(3, 4, requires_grad=True)\n",
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8152e-03, -7.8693e-02,  1.7513e+00,  1.6680e+01],\n",
       "        [ 1.8439e+01,  1.3424e-02,  9.0219e-02, -8.6612e-02],\n",
       "        [ 1.1599e+00,  1.3571e+00,  7.7540e+00,  4.3853e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(t.ones(y.size()))  # gradient形状与y一致\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8152e-03, -7.8693e-02,  1.7513e+00,  1.6680e+01],\n",
       "        [ 1.8439e+01,  1.3424e-02,  9.0219e-02, -8.6612e-02],\n",
       "        [ 1.1599e+00,  1.3571e+00,  7.7540e+00,  4.3853e+00]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autograd的计算结果与利用公式手动计算的结果一致\n",
    "gradf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 计算图\n",
    "\n",
    "PyTorch中`autograd`的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$ \\textbf {z = wx + b}$可分解为$\\textbf{y = wx}$和$\\textbf{z = y + b}$，其计算图如图3-3所示，图中`MUL`，`ADD`都是算子，$\\textbf{w}$，$\\textbf{x}$，$\\textbf{b}$即变量。\n",
    "\n",
    "![图3-3:computation graph](imgs/com_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上有向无环图中，$\\textbf{X}$和$\\textbf{b}$是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。$\\textbf{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。\n",
    "$${\\partial z \\over \\partial b} = 1,\\space {\\partial z \\over \\partial y} = 1\\\\\n",
    "{\\partial y \\over \\partial w }= x,{\\partial y \\over \\partial x}= w\\\\\n",
    "{\\partial z \\over \\partial x}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial x}=1 * w\\\\\n",
    "{\\partial z \\over \\partial w}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial w}=1 * x\\\\\n",
    "$$\n",
    "而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其过程如图3-4所示。\n",
    "\n",
    "![图3-4：计算图的反向传播](imgs/com_graph_backward.svg)\n",
    "\n",
    "\n",
    "在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作`Function`，每一个变量在图中的位置可通过其`grad_fn`属性在图中的位置推测得到。在反向传播过程中，autograd沿着这个图从当前变量（根节点$\\textbf{z}$）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以`Backward`结尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.ones(1)\n",
    "b = t.rand(1, requires_grad=True)\n",
    "w = t.rand(1, requires_grad=True)\n",
    "y = w * x  # 等价于y=w.mul(x)\n",
    "z = y + b  # 等价于z=y.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad, b.requires_grad, w.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w\n",
    "# 故而y.requires_grad为True\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf, w.is_leaf, b.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf, z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7fcdaa937080>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_fn可以查看这个variable的反向传播函数，\n",
    "# z是add函数的输出，所以它的反向传播函数是AddBackward\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7fcdaaabacc0>, 0),\n",
       " (<AccumulateGrad at 0x7fcdaaaba278>, 0))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next_functions保存grad_fn的输入，是一个tuple，tuple的元素也是Function\n",
    "# 第一个是y，它是乘法(mul)的输出，所以对应的反向传播函数y.grad_fn是MulBackward\n",
    "# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有\n",
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable的grad_fn对应着和图中的function相对应\n",
    "z.grad_fn.next_functions[0][0] == y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7fcdaaa53ba8>, 0), (None, 0))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个是w，叶子节点，需要求导，梯度是累加的\n",
    "# 第二个是x，叶子节点，不需要求导，所以为None\n",
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 叶子节点的grad_fn是None\n",
    "w.grad_fn, x.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算w的梯度的时候，需要用到x的数值(${\\partial y\\over \\partial w} = x $)，这些数值在**前向**过程中会保存成`buffer`，在**计算完梯度之后会自动清空**。为了能够多次反向传播需要指定`retain_graph`来**保留**这些buffer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用retain_graph来保存buffer\n",
    "z.backward(retain_graph=True)\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义\n",
    "z.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch使用的是**动态图**，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）**根据需求创建计算图**。这点在**自然语言处理**领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def abs(x):\n",
    "    if x.data[0] > 0: return x\n",
    "    else: return -x\n",
    "x = t.ones(1, requires_grad=True)\n",
    "y = abs(x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "x = -1 * t.ones(1)\n",
    "x = x.requires_grad_()\n",
    "y = abs(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 6., 3., 2.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    result = 1\n",
    "    for ii in x:\n",
    "        if ii.item() > 0: result = ii * result\n",
    "    return result\n",
    "x = t.arange(-2, 4, requires_grad=True, dtype=t.float)\n",
    "y = f(x)  # y = x[3]*x[4]*x[5]\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量的`requires_grad`属性**默认为False**，如果某一个节点requires_grad被设置为True，那么**所有依赖它的节点**`requires_grad`都是True。这其实很好理解，对于$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$，x.requires_grad = True，当需要计算$\\partial z \\over \\partial x$时，根据链式法则，$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$，自然也需要求$ \\frac{\\partial z}{\\partial y}$，所以y.requires_grad会被自动标为True. \n",
    "\n",
    "\n",
    "有些时候我们可能不希望autograd对tensor求导。认为求导需要缓存许多中间结构，增加额外的内存/显存开销，那么我们可以关闭自动求导。对于不需要反向传播的情景（如inference，即测试推理时），关闭自动求导可实现一定程度的速度提升，并节省约一半显存，因其不需要分配空间计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.ones(1, requires_grad=True)\n",
    "w = t.rand(1, requires_grad=True)\n",
    "y = x * w\n",
    "# y 依赖于 w，而 w.requires_grad = True\n",
    "x.requires_grad, w.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with t.no_grad():\n",
    "    x = t.ones(1)\n",
    "    w = t.rand(1, requires_grad=True)\n",
    "    y = x * w\n",
    "# y依赖于w和x，虽然 w.requires_grad = True，但是y的requires_grad依旧为False\n",
    "x.requires_grad, w.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nContext-manager that disabled gradient calculation.\\n\\n    Disabling gradient calculation is useful for inference, when you are sure\\n    that you will not call :meth:`Tensor.backward()`. It will reduce memory\\n    consumption for computations that would otherwise have `requires_grad=True`.\\n    In this mode, the result of every computation will have\\n    `requires_grad=False`, even when the inputs have `requires_grad=True`.\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "x = t.ones(1)\n",
    "w = t.rand(1, requires_grad=True)\n",
    "y = x * w\n",
    "# y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False\n",
    "x.requires_grad, w.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fcdaa91c8d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 恢复默认配置\n",
    "t.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改tensor的数值，但是又不希望被autograd记录，那么我么可以对tensor.data进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.ones(3, 4, requires_grad=True)\n",
    "b = t.ones(3, 4, requires_grad=True)\n",
    "c = a * b\n",
    "\n",
    "a.data # 还是一个tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data.requires_grad  # 但是已经是独立于计算图之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a.data.sigmoid_()  # sigmoid_ 是个inplace操作，会修改a自身的值\n",
    "d.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们希望对tensor，但是又不希望被记录, 可以使用tensor.data 或者tensor.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 近似于 tensor=a.data, 但是如果tensor被修改，backward可能会报错\n",
    "tensor = a.detach()\n",
    "tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计tensor的一些指标，不希望被记录\n",
    "mean = tensor.mean()\n",
    "std = tensor.std()\n",
    "maxium = tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[0] = 1\n",
    "# 下面会报错：　RuntimeError: one of the variables needed for gradient\n",
    "#             computation has been modified by an inplace operation\n",
    "#　因为 c=a*b, b的梯度取决于a，现在修改了tensor，其实也就是修改了a，梯度不再准确\n",
    "# c.sum().backward() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：\n",
    "- 使用autograd.grad函数\n",
    "- 使用hook\n",
    "\n",
    "`autograd.grad`和`hook`方法都是很强大的工具，更详细的用法参考官方api文档，这里举例说明基础的使用。推荐使用`hook`方法，但是在实际使用中应尽量避免修改grad的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.ones(3, requires_grad=True)\n",
    "w = t.rand(3, requires_grad=True)\n",
    "y = x * w\n",
    "# y 依赖于 w，而 w.requires_grad = True\n",
    "z = y.sum()\n",
    "x.requires_grad, w.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1534, 0.9236, 0.5543]), tensor([1., 1., 1.]), None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非叶子节点 grad 计算完之后自动清空，y.grad 是None\n",
    "z.backward()\n",
    "(x.grad, w.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]),)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一种方法：使用grad获取中间变量的梯度\n",
    "x = t.ones(3, requires_grad=True)\n",
    "w = t.rand(3, requires_grad=True)\n",
    "y = x * w\n",
    "z = y.sum()\n",
    "# z对y的梯度，隐式调用backward() dz/dy\n",
    "t.autograd.grad(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y的梯度： tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 第二种方法：使用hook\n",
    "# hook是一个函数，输入是梯度，不应该有返回值\n",
    "def variable_hook(grad):\n",
    "    print('y的梯度：',grad)\n",
    "\n",
    "x = t.ones(3, requires_grad=True)\n",
    "w = t.rand(3, requires_grad=True)\n",
    "y = x * w\n",
    "# 注册hook\n",
    "hook_handle = y.register_hook(variable_hook)\n",
    "z = y.sum()\n",
    "z.backward()  # 钩子在此时起作用！\n",
    "\n",
    "# 除非你每次都要用hook，否则用完之后记得移除hook\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后再来看看`variable`中`grad`属性和`backward`函数`grad_variables`参数的含义，这里直接下结论：\n",
    "\n",
    "- variable $\\textbf{x}$的梯度是目标函数${f(x)} $对$\\textbf{x}$的梯度，$\\frac{df(x)}{dx} = (\\frac {df(x)}{dx_0},\\frac {df(x)}{dx_1},...,\\frac {df(x)}{dx_N})$，形状和$\\textbf{x}$一致。\n",
    "- 对于y.backward(grad_variables)中的grad_variables相当于链式求导法则中的$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$中的$\\frac{\\partial z}{\\partial y}$。z是目标函数，一般是一个标量，故而$\\frac{\\partial z}{\\partial y}$的形状与variable $\\textbf{y}$的形状一致。`z.backward()`在一定程度上等价于y.backward(grad_y)。`z.backward()`省略了grad_variables参数，是因为$z$是一个标量，而$\\frac{\\partial z}{\\partial z} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,3, requires_grad=True, dtype=t.float)\n",
    "print(x)\n",
    "y = x**2 + x*2\n",
    "z = y.sum()\n",
    "z.backward()  # 从z开始反向传播\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,3, requires_grad=True, dtype=t.float)\n",
    "y = x**2 + x*2\n",
    "z = y.sum()\n",
    "y_gradient = t.Tensor([1,1,1])  # dz/dy\n",
    "y.backward(y_gradient)  #从y开始反向传播\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,3, requires_grad=True, dtype=t.float)\n",
    "y = x**2 + x*2\n",
    "y.backward(y_gradient)  #从y开始反向传播\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外值得注意的是，只有对variable的操作才能使用autograd，如果对variable的data直接进行操作，将无法使用反向传播。除了对参数初始化，一般我们不会修改variable.data的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中计算图的特点可总结如下：\n",
    "\n",
    "- autograd根据用户对variable的操作构建其计算图。对变量的操作抽象为`Function`。\n",
    "- 对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的`grad_fn`为None。叶子节点中需要求导的variable，具有`AccumulateGrad`标识，因其梯度是累加的。\n",
    "- variable默认是不需要求导的，即`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都为True。\n",
    "- variable的`volatile`属性默认为False，如果某一个variable的`volatile`属性被设为True，那么所有依赖它的节点`volatile`属性都为True。volatile属性为True的节点不会求导，volatile的优先级比`requires_grad`高。\n",
    "- 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定`retain_graph`=True来保存这些缓存。\n",
    "- 非叶子节点的梯度计算完之后即被清空，可以使用`autograd.grad`或`hook`技术获取非叶子节点的值。\n",
    "- variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播\n",
    "- 反向传播函数`backward`的参数`grad_variables`可以看成链式求导的中间结果，如果是标量，可以省略，默认为1\n",
    "- PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。\n",
    "\n",
    "这些知识不懂大多数情况下也不会影响对pytorch的使用，但是掌握这些知识有助于更好的理解pytorch，并有效的避开很多陷阱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 扩展autograd\n",
    "\n",
    "\n",
    "目前绝大多数函数都可以使用`autograd`实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办? 写一个`Function`，实现它的前向传播和反向传播代码，`Function`对应于计算图中的矩形， 它接收参数，计算并返回结果。下面给出一个例子。\n",
    "\n",
    "```python\n",
    "\n",
    "class Mul(Function):\n",
    "                                                            \n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b, x_requires_grad = True):\n",
    "        ctx.x_requires_grad = x_requires_grad\n",
    "        ctx.save_for_backward(w,x)\n",
    "        output = w * x + b\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        w,x = ctx.saved_tensors\n",
    "        grad_w = grad_output * x\n",
    "        if ctx.x_requires_grad:\n",
    "            grad_x = grad_output * w\n",
    "        else:\n",
    "            grad_x = None\n",
    "        grad_b = grad_output * 1\n",
    "        return grad_w, grad_x, grad_b, None\n",
    "```\n",
    "\n",
    "分析如下：\n",
    "\n",
    "- 自定义的`Function`需要继承`autograd.Function`，没有构造函数`__init__`，forward和backward函数都是静态方法\n",
    "- `backward`函数的输出和`forward`函数的输入一一对应，`backward`函数的输入和`forward`函数的输出一一对应\n",
    "- backward函数的grad_output参数即`t.autograd.backward`中的`grad_variables`\n",
    "- 如果某一个输入不需要求导，直接返回None，如forward中的输入参数`x_requires_grad`显然无法对它求导，直接返回None即可\n",
    "- 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放\n",
    "\n",
    "Function的使用利用`Function.apply(variable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class MultiplyAdd(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b):\n",
    "        ctx.save_for_backward(w, x)\n",
    "        output = w * x + b\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):                         \n",
    "        w,x = ctx.saved_tensors\n",
    "        grad_w = grad_output * x\n",
    "        grad_x = grad_output * w\n",
    "        grad_b = grad_output * 1\n",
    "        return grad_w, grad_x, grad_b   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, tensor([1.]), tensor([1.]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.ones(1)\n",
    "w = t.rand(1, requires_grad = True)\n",
    "b = t.rand(1, requires_grad = True)\n",
    "# 开始前向传播\n",
    "z = MultiplyAdd.apply(w, x, b)\n",
    "# 开始反向传播\n",
    "z.backward()\n",
    "\n",
    "# x不需要求导，中间过程还是会计算它的导数，但随后被清空\n",
    "x.grad, w.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([0.7731], grad_fn=<MulBackward0>), tensor([1.]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.ones(1)\n",
    "w = t.rand(1, requires_grad = True)\n",
    "b = t.rand(1, requires_grad = True)\n",
    "# print('开始前向传播')\n",
    "z = MultiplyAdd.apply(w,x,b)\n",
    "# print('开始反向传播')\n",
    "\n",
    "# 调用 MultiplyAdd.backward\n",
    "# 输出 grad_w, grad_x, grad_b\n",
    "z.grad_fn.apply(t.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之所以forward函数的输入是tensor，而backward函数的输入是variable，是为了实现高阶求导。backward函数的输入输出虽然是variable，但在实际使用时`autograd.Function`会将输入variable提取为tensor，并将计算结果的tensor封装成variable返回。在backward函数中，之所以也要对variable进行操作，是为了能够计算梯度的梯度（backward of backward）。下面举例说明，有关`torch.autograd.grad`的更详细使用请参照文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.], grad_fn=<MulBackward0>),)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.tensor([5.], requires_grad=True)\n",
    "y = x ** 2\n",
    "grad_x = t.autograd.grad(y, x, create_graph=True)\n",
    "grad_x  # dy/dx = 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.]),)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_grad_x = t.autograd.grad(grad_x[0],x)\n",
    "grad_grad_x  # 二阶导数 d(2x)/dx = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种设计虽然能让`autograd`具有高阶求导功能，但其也限制了Tensor的使用，因autograd中反向传播的函数只能利用当前已经有的Variable操作。这个设计是在`0.2`版本新加入的，为了更好的灵活性，也为了兼容旧版本的代码，PyTorch还提供了另外一种扩展autograd的方法。PyTorch提供了一个装饰器`@once_differentiable`，能够在backward函数中自动将输入的variable提取成tensor，把计算结果的tensor自动封装成variable。有了这个特性我们就能够很方便的使用numpy/scipy中的函数，操作不再局限于variable所支持的操作。但是这种做法正如名字中所暗示的那样只能求导一次，它打断了反向传播图，不再支持高阶求导。\n",
    "\n",
    "\n",
    "上面所描述的都是新式Function，还有个legacy Function，可以带有`__init__`方法，`forward`和`backwad`函数也不需要声明为`@staticmethod`，但随着版本更迭，此类Function将越来越少遇到，在此不做更多介绍。\n",
    "\n",
    "此外在实现了自己的Function之后，还可以使用`gradcheck`函数来检测实现是否正确。`gradcheck`通过数值逼近来计算梯度，可能具有一定的误差，通过控制`eps`的大小可以控制容忍的误差。\n",
    "关于这部份的内容可以参考github上开发者们的讨论。\n",
    "\n",
    "https://github.com/pytorch/pytorch/pull/1016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "                                                             \n",
    "    @staticmethod\n",
    "    def forward(ctx, x): \n",
    "        output = 1 / (1 + t.exp(-x))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): \n",
    "        output,  = ctx.saved_tensors\n",
    "        grad_x = output * (1 - output) * grad_output\n",
    "        return grad_x                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaoshuai/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/gradcheck.py:170: UserWarning: At least one of the inputs that requires gradient is not of double precision floating point. This check will likely fail if all the inputs are not of double precision floating point. \n",
      "  'At least one of the inputs that requires gradient '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 采用数值逼近方式检验计算梯度的公式对不对\n",
    "test_input = t.randn(3,4, requires_grad=True)\n",
    "t.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 µs ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "162 µs ± 7.88 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "105 µs ± 24.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def f_sigmoid(x):\n",
    "    y = Sigmoid.apply(x)\n",
    "    y.backward(t.ones(x.size()))\n",
    "    \n",
    "def f_naive(x):\n",
    "    y =  1/(1 + t.exp(-x))\n",
    "    y.backward(t.ones(x.size()))\n",
    "    \n",
    "def f_th(x):\n",
    "    y = t.sigmoid(x)\n",
    "    y.backward(t.ones(x.size()))\n",
    "    \n",
    "x=t.randn(100, 100, requires_grad=True)\n",
    "%timeit -n 100 f_sigmoid(x)\n",
    "%timeit -n 100 f_naive(x)\n",
    "%timeit -n 100 f_th(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然`f_sigmoid`要比单纯利用`autograd`加减和乘方操作实现的函数快不少，因为f_sigmoid的backward优化了反向传播的过程。另外可以看出系统实现的built-in接口(t.sigmoid)更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 小试牛刀: 用Variable实现线性回归\n",
    "在上一节中讲解了利用tensor实现线性回归，在这一小节中，将讲解如何利用autograd/Variable实现线性回归，以此感受autograd的便捷之处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子，为了在不同人电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000) \n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' 产生随机数据：y = x*2 + 3，加上了一些噪声'''\n",
    "    x = t.rand(batch_size,1) * 5\n",
    "    y = x * 2 + 3 + t.randn(batch_size, 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9850057363510132 3.0050430297851562\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1,1, requires_grad=True)\n",
    "b = t.zeros(1,1, requires_grad=True)\n",
    "losses = np.zeros(500)\n",
    "\n",
    "lr =0.005 # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=32)\n",
    "    \n",
    "    # forward：计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y)\n",
    "    loss = 0.5 * (y_pred - y) ** 2\n",
    "    loss = loss.sum()\n",
    "    losses[ii] = loss.item()\n",
    "    \n",
    "    # backward：手动计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.data.sub_(lr * w.grad.data)\n",
    "    b.data.sub_(lr * b.grad.data)\n",
    "    \n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    if ii%50 ==0:\n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 6, dtype=t.float).view(-1, 1)\n",
    "        y = x.mm(w.data) + b.data.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=20) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        plt.xlim(0,5)\n",
    "        plt.ylim(0,13)   \n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print(w.item(), b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylim(5,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用autograd实现的线性回归最大的不同点就在于autograd不需要计算反向传播，可以自动计算微分。这点不单是在深度学习，在许多机器学习的问题中都很有用。另外需要注意的是在每次反向传播之前要记得先把梯度清零。\n",
    "\n",
    "本章主要介绍了PyTorch中两个基础底层的数据结构：Tensor和autograd中的Variable。Tensor是一个类似Numpy数组的高效多维数值运算数据结构，有着和Numpy相类似的接口，并提供简单易用的GPU加速。Variable是autograd封装了Tensor并提供自动求导技术的，具有和Tensor几乎一样的接口。`autograd`是PyTorch的自动微分引擎，采用动态计算图技术，能够快速高效的计算导数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
